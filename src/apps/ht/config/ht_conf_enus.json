{
    "_json_description": "Json para la configuracion de textos estaticos, de la aplicacion ht",
    "_json_version": "0.3.0",
    "pages": {
        "main_page": {
            "title": "High Thoughput Collection",
            "descripcion": "",
            "_collection_coments": "El elemento 'collection' es un arreglo donde se pueden declarar las colecciones, cuidar que el identificador sea diferente. ",
            "collection": [{
                    "id": "DS-001",
                    "url": "TFBINDING",
                    "title": "TF Binding Sites",
                    "description": " The high-throughput transcription factor binding sites (TFBS) dataset collection is composed of datasets generated by ChIP-seq experiments. \n**Summary**\nThis collection contains 28 datasets related to 12 different TFs. \n **Creation of the collection** \n1. PubMed is searched for articles relating high-throughput experiments, and specifically using the ChIP-seq technology. The articles are carefully selected and curated.\n2. **Annotation process** \n   a. The data reported in the curated articles is reported and detailed in a dataset registry, where a dataset is composed of a given sample and its technical and/or biological replicates, when available, as well as the targeted TF and the experimental condition (organism, strain, temperature, medium, medium supplements, pH, etc). Each dataset is also associated with its “series ID” (whether from GEO or ArrayExpress), the experiment platform, the article’s PMID, the reference genome, and the control samples if provided. \n    b. The supplementary files reported by the authors are downloaded, specifically the identified binding sites and their coordinates. These files are formatted into a pre-defined model.\n    c. The curation process associates the TFBS files with their corresponding dataset, TF and associated metadata. \n3. **Uniformization**. This step allows the standardization of all the datasets in the collection. \n    a. Starting with the dataset table, series and sample IDs are extracted in order to automatize the downloading of all of the raw Fastq files composing each dataset (treatment sample, control, replicates). \n    b. The raw data is pre-processed: each sample is independently cleaned, trimmed, mapped to the reference genome, and sequencing quality is verified. The peak-calling step is performed, using the aligned samples and their controls. It consists in detecting regions of the genome where a significant amount of reads were mapped, likely to indicate a binding position. This step generates a single, bed-formatted peak file for each dataset.\n    c. The post-processing consists in identifying actual binding sites inside of the peaks regions. For each dataset, the TF motif available in RegulonDB is used for pattern-matching using RSAT matrix-scan. When a known motif is not available, a putative one is generated through *de novo* motif search in the peaks regions using RSAT peak-motifs. The identified binding sites are then used to build a new binding matrix, specific to each dataset. \n4. **Mapping with RegulonDB and other datasets**. RegulonDB currently holds the biggest collection of transcriptional regulatory binding sites for E. coli. Binding sites identified in the ChIP-seq datasets are compared to the RegulonDB collection, in order to tag those as formerly known or potentially new. ChIP-seq datasets of a given TF are also compared to each other. Finally, all binding sites are associated with their closest gene(s) and its product. ",
                    "enable": true
                },
                {
                    "id": "DS-002",
                    "url": "TUS",
                    "title": "Transcription Units",
                    "description": "### Transcription Units\n**Summary**\nThis collection contains 5 datasets.\n\n **Creation of the collection**\n**1. Annotation**\nOriginal data separated by growth conditions.\n\n**2. Uniformization**\n TUs were collected from 3 curated publications, and processed to produce uniform bed-like dataset files with the following fields:\n\n- __chromosome:__ NC_000913.3\n- __start:__ Left genomic position\n- __stop:__ Right genomic position\n- __id:__ Unique TU ID\n- __length:__ TU length\n- __strand:__ TU strand reported by authors\n- __term_type:__ depending on methodology\n- __gene_number:__ number of genes entirely contained in TU\n- __genes:__ bnumbers of genes entirely contained in TU\n- __pseudo:__ 1 if TU contains pseudo genes, else 0\n- __phantom:__ 1 if TU contains phantom genes, else 0\n\nWhen the original data was reported to have coordinates from the NC_000913.2 version of the genome, they were converted to the latest version NC_000913.3 using the online converter provided by biocyc: https://biocyc.org/ECOLI/map-seq-coords-form?chromosome=COLI-K12 \n\nThe genes reported in those datasets may differ from the genes originally reported by the authors. Here, we decided to include only the genes which are entirely contained in the TU coordinates, and exclude the others. \n",
                    "enable": true
                },
                {
                    "id": "DS-003",
                    "url": "HT-TTs",
                    "title": "Transcription Termination Sites",
                    "description": "### Transcription Terminator Sites\n**Summary**\nThis collection contains 5 datasets.\n\n**Methodology**\n\n__1. Annotation__\nOriginal data separated by growth conditions.\n__2. Uniformization__\nTTSs were collected from 3 curated publications, and processed to produce uniform bed-like dataset files with the following fields:\n\n- __chromosome:__ NC_000913.3\n- __start:__ Left genomic position. If not reported, will be the same as term_pos\n- __stop:__ Right genomic position. If not reported, will be the same as term_pos\n- __id:__ Unique TSS ID\n- __term_pos:__ Terminal position reported by authors\n- __strand:__ TTS strand reported by authors\n\nWhen the original data was reported to have coordinates from the NC_000913.2 version of the genome, they were converted to the latest version NC_000913.3 using the online converter provided by biocyc: https://biocyc.org/ECOLI/map-seq-coords-form?chromosome=COLI-K12 ",
                    "enable": false
                },
                {
                    "id": "DS-004",
                    "url": "HT-TSS",
                    "title": "Transcription Start Sites",
                    "description": "### Transcription Start Sites\n__Summary__\nThis collection contains 15 datasets.\n\n__Creation of the collection__\n**1. Annotation**\nOriginal data separated by growth conditions\n\n**2. Uniformization**\nTSSs were collected from 7 curated publications, and processed to produce uniform bed-like dataset files with the following fields:\n\n- __chromosome:__ NC_000913.3\n- __start:__ Left genomic position. If not reported, will be the same as pos_1\n- __stop:__ Right genomic position. If not reported, will be the same as pos_1\n- __id:__ Unique TSS ID\n- __pos_1:__ TSS +1 position reported by authors\n- __strand:__ TSS strand reported by authors\n\nWhen the original data was reported to have coordinates from the NC_000913.2 version of the genome, they were converted to the latest version NC_000913.3 using the online converter provided by biocyc: https://biocyc.org/ECOLI/map-seq-coords-form?chromosome=COLI-K12 ",
                    "enable": false
                },
                {
                    "id": "DS-005",
                    "url": "HT-GE",
                    "title": "Gene Expression",
                    "description": "### Gene Expression\n__Experiment selection__\nWe downloaded experiments in four batches over a period of approximately 2 years. Our first batch consisted of 1277 experiments and it was gathered by the manual curation process regularly done by the RegulonDB team. Shortly, original scientific papers about transcriptional regulation in E. coli K-12 are monthly sought, selected, and curated as described previously [Santos-Zavaleta, A., et al. 2019 30395280]; the search was completed by querying directly the ArrayExpress database, where a few datasets were found that are not associated with any publication. The second batch was obtained from dee2.io (Digital Expression Explorer), and covered 1255 experiments that were not present in the initial manual curation. The third batch was downloaded in early 2020 and contained 37 new expression datasets we had not previously retrieved, and our final batch was downloaded in October 2021, and contained 115 experiments published since the third batch.\n\n__Annotation__\nFor experiments that are present in the Gene Expression Omnibus (GEO), we use Natural Language Processing (NLP) in order to extract experimental metadata to describe experiments (NLP extraction section).  For those experiments that are only present in SRA, we used NCBI's Entrez tool, along with custom software, to gather the metadata.  In particular, we used the python package Beautiful Soup 4 in order to perform web-scraping where metadata was missing, or poorly annotated in Entrez. \n\n__Data gathering__\nWe initially filtered these datasets based on the type of experiment and sequencing format used, retaining only RNA-seq experiments, and removing those performed with SOLiD sequencing, as our pipeline is tailored towards Illumina sequencing. This collection is up to date as of the end of October 2021.  We then downloaded the fastq files for each of these datasets to be homogeneously processed by our sequence analysis pipeline.\n\n__Data processing and normalization__\nWe aligned all samples to the E.coli reference genome U00096.3 using HISAT2. Our alignments are always run as unpaired; and when the metadata allows determination of the library preparation kit used, we provide the appropriate strandedness parameters, which indicates whether reads are to be expected on the same, or opposite strand of the mRNA transcript.  To ensure high-quality comparisons using these public expression data, we filtered out some of the samples based on alignment metrics.  We eliminated those samples with less than 5-million raw reads, and those with less than 90% of their reads aligned to the E.coli reference genome.  We also eliminated samples with less than 90% of genes with non-zero coverage. We then perform DEseq-normalization to facilitate comparisons across different datasets.  Shortly, we created a \"pseudo-reference\" sample, where we obtained the geometric mean of each gene's expression, measured each in counts, fpkm, and tpm.  Each gene in a given sample was divided by its pseudo-reference value, and a scaling factor for each sample was obtained by taking the median of these values. The final DEseq-normalized values are obtained by dividing each sample's expression by the sample scaling factor.",
                    "enable": false
                }
            ]
        },
        "builder_page": {
            "title": "Query Builder",
            "description": ""
        },
        "result_page": {
            "title": "Results of",
            "description": ""
        },
        "dataset_page": {
            "description": "descripcion",
            "sections": {
                "dataset_info": {
                    "title": "DATASET",
                    "description": ""
                },
                "dataset_tf": {
                    "title": "TRANSCRIPTION FACTOR",
                    "description": ""
                },
                "dataset_growthc": {
                    "title": "GROWTH CONDITION",
                    "description": ""
                },
                "dataset_author_data": {
                    "title": "AUTHOR DATA",
                    "description": ""
                },
                "dataset_igv": {
                    "title": "GENOME VIEWER",
                    "description": ""
                }
            }
        }
    }
}
